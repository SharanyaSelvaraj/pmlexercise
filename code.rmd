---
author: "Sharanya Selvaraj"
date: "12 May 2020"
output:
  html_document:
    keep_md: yes
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5)
options(width=120)
library(plyr)
library(dplyr)
library(randomForest)
library(rpart)
library(AppliedPredictiveModeling)
library(caret)
library(ElemStatLearn)
library(pgmm)
library(rpart)
library(gbm)
library(lubridate)
library(forecast)
library(e1071)
library(mgcv)
library(nlme)
library(MASS)
library(elasticnet)
library(lars)
set.seed(19790811)
```
Load the test and train data set

```{r}
library(ElemStatLearn)
data(pml-training.train)
data(pml-testing.test)
```
```{r}
Set the seed value as 1234. load the training and testing dataset.
training<- read.csv("D:/PMLC/pml-training.csv", na.strings=c("NA","#DIV/0!",""))
 testing <- read.csv("D:/PMLC/pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
```
```{r}
#Create testing and training as 60-20. 
inTrain<- createDataPartition(y=training$classe, p=0.6, list=FALSE)
mTraining<- training[inTrain, ]
myTesting<- training[-inTrain, ]

#apply 10 fold cross crossvalidation to training
train.control <- trainControl(method = "cv", number = 10, repetitions=3)


dim(myTraining);
11776   160
dim(myTesting);
7846  160
```
# Clean the data find near zero values and eliminate it

myDataNZV<- nearZeroVar(myTraining, saveMetrics=TRUE)
>myNZVvars<- names(myTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
+ "kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
+ "max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
+ "var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
+ "stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
+ "kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
+ "max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
+ "kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
+ "skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
+ "amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
+ "skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
+ "max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
+ "amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
+ "avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
+ "stddev_yaw_forearm", "var_yaw_forearm")
myTraining<- myTraining[!myNZVvars]
dim(myTraining)
[1] 11776   100

# Loop through each set of values
myTraining<- myTraining[c(-1)]
trainingV3 <- myTraining #creating another subset to iterate in loop
for(i in 1:length(myTraining)) { #for every column in the training dataset
+         if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .6 ) { #if n?? NAs > 60% of total observations
+         for(j in 1:length(trainingV3)) {
+             if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) ==1)  { #if the columns are the same:
+                 trainingV3 <- trainingV3[ , -j] #Remove that column
+             }   
+         } 
+     }
+ }

myTraining<- trainingV3
rm(trainingV3)
 clean1 <- colnames(myTraining)
 clean2 <- colnames(myTraining[, -58]) #already with classe column removed
myTesting<- myTesting[clean1]
 testing <- testing[clean2]



#Check the dimensions
dim(myTesting)
[1] 7846   58
dim(testing)
[1] 20 57

for (i in 1:length(testing) ) {
+         for(j in 1:length(myTraining)) {
+         if( length( grep(names(myTraining[i]), names(testing)[j]) ) ==1)  {
+             class(testing[j]) <- class(myTraining[i])
+         }      
+     }      
+ }

#note row 2 does not mean anything, this will be removed right.. now:
 testing <- rbind(myTraining[2, -58] , testing) 
testing <- testing[-1,]

#Fit the model and make predictions
modFitA1 <- rpart(classe ~ ., data=train.contol, method="class")
 predictionsA1 <- predict(modFitA1, train.contol, type = "class")
 
 #estimate the models performance throygh its confusion matrix
 confusionMatrix(predictionsA1, train.contol$classe)
Confusion Matrix and Statistics

          Reference
   Prediction    A    B    C    D    E
         A 2157   68   10    1    0
         B   60 1265   73   67    0
         C   15  177 1261  141   70
         D    0    8   15  962  111
         E    0    0    9  115 1261
Overall Statistics

Accuracy : 0.8802          
                 95% CI : (0.8728, 0.8873)
    No Information Rate : 0.2845          
    P-Value [Acc> NIR] : < 2.2e-16       

Kappa : 0.8484          

Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9664   0.8333   0.9218   0.7481   0.8745
Specificity            0.9859   0.9684   0.9378   0.9796   0.9806
PosPred Value         0.9647   0.8635   0.7578   0.8777   0.9105
NegPred Value         0.9866   0.9604   0.9827   0.9520   0.9720
Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2749   0.1612   0.1607   0.1226   0.1607
Detection Prevalence   0.2850   0.1867   0.2121   0.1397   0.1765
Balanced Accuracy      0.9762   0.9009   0.9298   0.8638   0.9276

#prediction using random forest
# install random forest
library(randomForest)
randomForest 4.6-14
Type rfNews() to see new features/changes/bug fixes.
Attaching package: ‘randomForest’
The following object is masked from ‘package:ggplot2’:
margin

 # Fit the model
 modFitB1 <- randomForest(classe ~. , data=train.control)
predictionsB1 <- predict(modFitB1, train.control, type = "class")

#Evaluate the performance by random forest
confusionMatrix(predictionsB1, train.control$classe)
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 2232    5    0    0    0
         B    0 1513    2    0    0
         C    0    0 1362    7    0
         D    0    0    4 1278    1
         E    0    0    0    1 1441

Overall Statistics

Accuracy : 0.9975          
                 95% CI : (0.9961, 0.9984)
    No Information Rate : 0.2845          
    P-Value [Acc> NIR] : < 2.2e-16       

Kappa : 0.9968          

Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            1.0000   0.9967   0.9956   0.9938   0.9993
Specificity            0.9991   0.9997   0.9989   0.9992   0.9998
PosPred Value         0.9978   0.9987   0.9949   0.9961   0.9993
NegPred Value         1.0000   0.9992   0.9991   0.9988   0.9998
Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2845   0.1928   0.1736   0.1629   0.1837
Detection Prevalence   0.2851   0.1931   0.1745   0.1635   0.1838
Balanced Accuracy      0.9996   0.9982   0.9973   0.9965   0.9996

# Write the predictions to a file B2
predictionsB2 <- predict(modFitB1, testing, type = "class")
>pml_write_files = function(x){
+   n = length(x)
+   for(i in 1:n){
+     filename = paste0("problem_id_",i,".txt")
+     write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
+   }
+ }
>pml_write_files(predictionsB2)
# Read the file B2
 predictionsB2
1  2  3 41  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
 B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  BB
Levels: A B C D E



